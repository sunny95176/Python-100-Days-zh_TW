## 併發程式設計在爬蟲中的應用

之前的課程，我們已經為大家介紹了 Python 中的多執行緒、多程序和非同步程式設計，透過這三種手段，我們可以實現併發或並行程式設計，這一方面可以加速程式碼的執行，另一方面也可以帶來更好的使用者體驗。爬蟲程式是典型的 I/O 密集型任務，對於 I/O 密集型任務來說，多執行緒和非同步 I/O 都是很好的選擇，因為當程式的某個部分因 I/O 操作阻塞時，程式的其他部分仍然可以運轉，這樣我們不用在等待和阻塞中浪費大量的時間。下面我們以爬取“[360圖片](https://image.so.com/)”網站的圖片並儲存到本地為例，為大家分別展示使用單執行緒、多執行緒和非同步 I/O 程式設計的爬蟲程式有什麼區別，同時也對它們的執行效率進行簡單的對比。

“360圖片”網站的頁面使用了 [Ajax](https://developer.mozilla.org/zh-CN/docs/Web/Guide/AJAX) 技術，這是很多網站都會使用的一種非同步載入資料和區域性重新整理頁面的技術。簡單的說，頁面上的圖片都是透過 JavaScript 程式碼非同步獲取 JSON 資料並動態渲染生成的，而且整個頁面還使用了瀑布式載入（一邊向下滾動，一邊載入更多的圖片）。我們在瀏覽器的“開發者工具”中可以找到提供動態內容的資料介面，如下圖所示，我們需要的圖片資訊就在伺服器返回的 JSON 資料中。

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211205221352.png" style="zoom:50%;">

例如，要獲取“美女”頻道的圖片，我們可以請求如下所示的URL，其中引數`ch`表示請求的頻道，`=`後面的引數值`beauty`就代表了“美女”頻道，引數`sn`相當於是頁碼，`0`表示第一頁（共`30`張圖片），`30`表示第二頁，`60`表示第三頁，以此類推。

```
https://image.so.com/zjl?ch=beauty&sn=0
```

### 單執行緒版本

透過上面的 URL 下載“美女”頻道共`90`張圖片。

```Python
"""
example04.py - 單執行緒版本爬蟲
"""
import os

import requests


def download_picture(url):
    filename = url[url.rfind('/') + 1:]
    resp = requests.get(url)
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file:
            file.write(resp.content)


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    for page in range(3):
        resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}')
        if resp.status_code == 200:
            pic_dict_list = resp.json()['list']
            for pic_dict in pic_dict_list:
                download_picture(pic_dict['qhimg_url'])

if __name__ == '__main__':
    main()
```

在 macOS 或 Linux 系統上，我們可以使用`time`命令來了解上面程式碼的執行時間以及 CPU 的利用率，如下所示。

```Bash
time python3 example04.py 
```

下面是單執行緒爬蟲程式碼在我的電腦上執行的結果。

```
python3 example04.py  2.36s user 0.39s system 12% cpu 21.578 total
```

這裡我們只需要關注程式碼的總耗時為`21.578`秒，CPU 利用率為`12%`。

### 多執行緒版本

我們使用之前講到過的執行緒池技術，將上面的程式碼修改為多執行緒版本。

```Python
"""
example05.py - 多執行緒版本爬蟲
"""
import os
from concurrent.futures import ThreadPoolExecutor

import requests


def download_picture(url):
    filename = url[url.rfind('/') + 1:]
    resp = requests.get(url)
    if resp.status_code == 200:
        with open(f'images/beauty/{filename}', 'wb') as file:
            file.write(resp.content)


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    with ThreadPoolExecutor(max_workers=16) as pool:
        for page in range(3):
            resp = requests.get(f'https://image.so.com/zjl?ch=beauty&sn={page * 30}')
            if resp.status_code == 200:
                pic_dict_list = resp.json()['list']
                for pic_dict in pic_dict_list:
                    pool.submit(download_picture, pic_dict['qhimg_url'])


if __name__ == '__main__':
    main()
```

執行如下所示的命令。

```Bash
time python3 example05.py
```

程式碼的執行結果如下所示：

```
python3 example05.py  2.65s user 0.40s system 95% cpu 3.193 total
```

### 非同步I/O版本

我們使用`aiohttp`將上面的程式碼修改為非同步 I/O 的版本。為了以非同步 I/O 的方式實現網路資源的獲取和寫檔案操作，我們首先得安裝三方庫`aiohttp`和`aiofile`，命令如下所示。

```Bash
pip install aiohttp aiofile
```

`aiohttp` 的用法在之前的課程中已經做過簡要介紹，`aiofile`模組中的`async_open`函式跟 Python 內建函式`open`的用法大致相同，只不過它支援非同步操作。下面是非同步 I/O 版本的爬蟲程式碼。

```Python
"""
example06.py - 非同步I/O版本爬蟲
"""
import asyncio
import json
import os

import aiofile
import aiohttp


async def download_picture(session, url):
    filename = url[url.rfind('/') + 1:]
    async with session.get(url, ssl=False) as resp:
        if resp.status == 200:
            data = await resp.read()
            async with aiofile.async_open(f'images/beauty/{filename}', 'wb') as file:
                await file.write(data)


async def fetch_json():
    async with aiohttp.ClientSession() as session:
        for page in range(3):
            async with session.get(
                url=f'https://image.so.com/zjl?ch=beauty&sn={page * 30}',
                ssl=False
            ) as resp:
                if resp.status == 200:
                    json_str = await resp.text()
                    result = json.loads(json_str)
                    for pic_dict in result['list']:
                        await download_picture(session, pic_dict['qhimg_url'])


def main():
    if not os.path.exists('images/beauty'):
        os.makedirs('images/beauty')
    loop = asyncio.get_event_loop()
    loop.run_until_complete(fetch_json())
    loop.close()


if __name__ == '__main__':
    main()
```

執行如下所示的命令。

```Bash
time python3 example06.py
```

程式碼的執行結果如下所示：

```
python3 example06.py  0.82s user 0.21s system 27% cpu 3.782 total
```

### 總結

透過上面三段程式碼執行結果的比較，我們可以得出一個結論，使用多執行緒和非同步 I/O 都可以改善爬蟲程式的效能，因為我們不用將時間浪費在因 I/O 操作造成的等待和阻塞上，而`time`命令的執行結果也告訴我們，單執行緒的程式碼 CPU 利用率僅僅只有`12%`，而多執行緒版本的 CPU 利用率則高達`95%`；單執行緒版本的爬蟲執行時間約`21`秒，而多執行緒和非同步 I/O 的版本僅執行了`3`秒鐘。另外，在執行時間差別不大的情況下，多執行緒的程式碼比非同步 I/O 的程式碼耗費了更多的 CPU 資源，這是因為多執行緒的排程和切換也需要花費 CPU 時間。至此，三種方式在 I/O 密集型任務上的優劣已經一目瞭然，當然這只是在我的電腦上跑出來的結果。如果網路狀況不是很理想或者目標網站響應很慢，那麼使用多執行緒和非同步 I/O 的優勢將更為明顯，有興趣的讀者可以自行試驗。
